{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data as dt\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import fair_classification.utils as ut\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import data as dt\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, plot_model_comparison,\n",
    "    selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "    false_positive_rate, false_negative_rate,\n",
    "    false_positive_rate_difference, false_negative_rate_difference,\n",
    "    equalized_odds_difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores key value pairs of dataset, sensitive attributes and target labels\n",
    "DATA2D = {'adult': 'target',\n",
    "          'compas': 'ScoreText_',\n",
    "          'german': 'loan_status',\n",
    "          'synthetic' : 'D'}\n",
    "\n",
    "DATA2S = {'adult': 'sex',\n",
    "          'compas': 'Ethnic_Code_Text_',\n",
    "          'german': 'sex',\n",
    "          'synthetic': 'S'}\n",
    "\n",
    "NAMES = ['adult', 'compas', 'german', 'synthetic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(\"report.json\")\n",
    "config = None\n",
    "\n",
    "with open(config_path, 'r') as fh:\n",
    "    content = json.load(fh)\n",
    "\n",
    "\n",
    "\n",
    "dataset = content['dataset']\n",
    "exp_num = content['exp-id']\n",
    "fold = content['fold']\n",
    "num_X = content['num_X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, cloumns, learn_decision_label, train_y_fair, train_y_proxy, test_y_fair, test_y_proxy = dt.load_data(dataset, fold, num_X=num_X, use_fair=False, exp_num=exp_num)\n",
    "X_train = np.array(train_data.drop(columns=cloumns))\n",
    "X_test = np.array(test_data.drop(columns=cloumns))\n",
    "s_train = np.array(train_data[DATA2S[dataset]])\n",
    "protected_train = [s_train]\n",
    "s_test = np.array(test_data[DATA2S[dataset]])\n",
    "protected_test = [s_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {}\n",
    "ans = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fair Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(prob, true_label, sv):\n",
    "    try:\n",
    "        pred = (prob > 0.5).astype(int)\n",
    "        fair_metrics = {}\n",
    "        fair_metrics[\"accuracy\"] = dt.accuracy(prob, true_label)\n",
    "        fair_metrics[\"precision\"] = dt.precision(prob, true_label)\n",
    "        fair_metrics[\"f1_score\"] = f1_score(pred, true_label)\n",
    "        fair_metrics[\"recall\"] = dt.recall(prob, true_label)\n",
    "        fair_metrics[\"Overall selection rate\"] = selection_rate(true_label, pred)\n",
    "        fair_metrics[\"Demographic parity difference\"] = demographic_parity_difference(true_label, pred, sensitive_features=sv)\n",
    "        fair_metrics[\"Demographic parity ratio\"] = demographic_parity_ratio(true_label, pred, sensitive_features=sv)\n",
    "        fair_metrics[\"False positive rate difference\"] = false_positive_rate_difference(true_label, pred, sensitive_features=sv)\n",
    "        fair_metrics[\"Equalized odds difference\"] = equalized_odds_difference(true_label, pred, sensitive_features=sv)\n",
    "        fair_metrics[\"False negative rate difference\"] = false_negative_rate_difference(true_label, pred, sensitive_features=sv)\n",
    "        return fair_metrics\n",
    "\n",
    "    except TypeError as e:\n",
    "        print(f\"{prob=}, {true_label=}\")\n",
    "        print(f\"{type(prob)=} , {type(true_label)=}\")\n",
    "        print(\"error\")\n",
    "        print(f\"{e.with_traceback()=}\")\n",
    "    # print(fair_metrics)\n",
    "    # return pd.DataFrame.from_dict(fair_metrics, orient=\"index\", columns=fair_metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(models_dict, s_test):\n",
    "    res = {}\n",
    "    for name, models in models_dict.items():\n",
    "        res[name] = {}\n",
    "        print(name)\n",
    "        print(models.keys())\n",
    "        for exp, metrics in models.items():\n",
    "            print(exp)\n",
    "            res[name][exp] = {}\n",
    "            res[name][exp][\"proxy\"] = summary(metrics[\"prob_test\"], test_y_fair, s_test )\n",
    "            res[name][exp][\"fair\"] = summary(metrics[\"prob_test\"], test_y_proxy, s_test )\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REWEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reweight as rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01909891]\n",
      "Train Accuracy 0.1617283950617284\n",
      "Train Violation 0.0008263243323004543  \t\t All violations [0.0008263243323004543]\n",
      "Test Accuracy 0.44\n",
      "Test Violation 0.050000000000000044  \t\t All violations [-0.050000000000000044]\n",
      "\n",
      "\n",
      "[0.17457577]\n",
      "Train Accuracy 0.27037037037037037\n",
      "Train Violation 0.001003393832079147  \t\t All violations [0.001003393832079147]\n",
      "Test Accuracy 0.29\n",
      "Test Violation 0.0485714285714286  \t\t All violations [-0.0485714285714286]\n",
      "\n",
      "\n",
      "[-0.01909891]\n",
      "Train Accuracy 0.1617283950617284\n",
      "Train Violation 0.0008263243323004543  \t\t All violations [0.0008263243323004543]\n",
      "Test Accuracy 0.17\n",
      "Test Violation 0.050000000000000044  \t\t All violations [-0.050000000000000044]\n",
      "\n",
      "\n",
      "[0.17457577]\n",
      "Train Accuracy 0.27037037037037037\n",
      "Train Violation 0.001003393832079147  \t\t All violations [0.001003393832079147]\n",
      "Test Accuracy 0.3\n",
      "Test Violation 0.0485714285714286  \t\t All violations [-0.0485714285714286]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models_dict[\"reweight\"] = {}\n",
    "models_dict[\"reweight\"][\"pre\"] = rw.learning(X_train, train_y_fair, X_test, test_y_proxy, protected_train, protected_test)\n",
    "models_dict[\"reweight\"][\"post\"] = rw.learning(X_train, train_y_proxy, X_test, test_y_fair, protected_train, protected_test)\n",
    "models_dict[\"reweight\"][\"pre_post\"] = rw.learning(X_train, train_y_fair, X_test, test_y_fair, protected_train, protected_test)\n",
    "models_dict[\"reweight\"][\"proxy\"] = rw.learning(X_train, train_y_proxy, X_test, test_y_proxy, protected_train, protected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      True\n",
       "1      True\n",
       "2     False\n",
       "3     False\n",
       "4     False\n",
       "      ...  \n",
       "95     True\n",
       "96     True\n",
       "97     True\n",
       "98    False\n",
       "99    False\n",
       "Name: P(Df|e) test_x, Length: 100, dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_proxy == test_y_fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dict[\"reweight\"][\"pre\"][\"test_acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dict[\"reweight\"][\"proxy\"][\"test_acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dict[\"reweight\"][\"post\"][\"test_acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dict[\"reweight\"][\"pre_post\"][\"test_acc\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAIR LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fair_lr as flr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_control_test = {DATA2S[dataset]: s_test}\n",
    "x_control_train= {DATA2S[dataset]: s_train}\n",
    "\n",
    "lr_y_train_fair = np.array([-1 if y == 0 else 1 for y in train_y_fair])\n",
    "lr_y_train_proxy = np.array([-1 if y == 0 else 1 for y in train_y_proxy])\n",
    "lr_y_test_proxy = np.array([-1 if y == 0 else 1 for y in test_y_proxy])\n",
    "lr_y_test_fair = np.array([-1 if y == 0 else 1 for y in test_y_fair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "Protected/non-protected in +ve class: 69% / 71%\n",
      "P-rule achieved: 97%\n",
      "Covariance between sensitive feature and decision from distance boundary : 0.111\n",
      "<class 'dict'>\n",
      "Accuracy: 0.69\n",
      "Protected/non-protected in +ve class: 78% / 93%\n",
      "P-rule achieved: 84%\n",
      "Covariance between sensitive feature and decision from distance boundary : 0.062\n",
      "<class 'dict'>\n",
      "Accuracy: 0.85\n",
      "Protected/non-protected in +ve class: 69% / 71%\n",
      "P-rule achieved: 97%\n",
      "Covariance between sensitive feature and decision from distance boundary : 0.112\n",
      "<class 'dict'>\n",
      "Accuracy: 0.64\n",
      "Protected/non-protected in +ve class: 78% / 93%\n",
      "P-rule achieved: 84%\n",
      "Covariance between sensitive feature and decision from distance boundary : 0.062\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "models_dict[\"fair_lr\"] = {}\n",
    "models_dict[\"fair_lr\"][\"pre\"] = flr.model(X_train, lr_y_train_fair, x_control_train, X_test, lr_y_test_proxy, x_control_test, DATA2S[dataset])[2]\n",
    "models_dict[\"fair_lr\"][\"post\"] = flr.model(X_train, lr_y_train_proxy, x_control_train, X_test, lr_y_test_fair, x_control_test, DATA2S[dataset])[2]\n",
    "models_dict[\"fair_lr\"][\"pre_post\"] = flr.model(X_train, lr_y_train_fair, x_control_train, X_test, lr_y_test_fair, x_control_test, DATA2S[dataset])[2]\n",
    "models_dict[\"fair_lr\"][\"proxy\"] = flr.model(X_train, lr_y_train_proxy, x_control_train, X_test, lr_y_test_proxy, x_control_test, DATA2S[dataset])[2]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fair Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fair_reduction as fr\n",
    "from fairlearn.reductions import ExponentiatedGradient\n",
    "from fairlearn.reductions import DemographicParity\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction(y_label):\n",
    "    learn = ExponentiatedGradient(\n",
    "            LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "            constraints=DemographicParity())\n",
    "\n",
    "    learn.fit(X_train, y_label, sensitive_features=s_train)\n",
    "\n",
    "        # predict\n",
    "    prob_test = learn._pmf_predict(X_test)[:, 1]\n",
    "    prob_train = learn._pmf_predict(X_train)[:, 1]\n",
    "    res = {}\n",
    "    res[\"prob_train\"] = prob_test\n",
    "    res[\"prob_test\"] = prob_test\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict[\"reduction\"] = {}\n",
    "models_dict[\"reduction\"][\"pre\"] = reduction(train_y_fair)\n",
    "models_dict[\"reduction\"][\"post\"] = reduction(train_y_proxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reweight\n",
      "dict_keys(['pre', 'post', 'pre_post', 'proxy'])\n",
      "pre\n",
      "post\n",
      "pre_post\n",
      "proxy\n",
      "fair_lr\n",
      "dict_keys(['pre', 'post', 'pre_post', 'proxy'])\n",
      "pre\n",
      "post\n",
      "pre_post\n",
      "proxy\n",
      "reduction\n",
      "dict_keys(['pre', 'post'])\n",
      "pre\n",
      "post\n"
     ]
    }
   ],
   "source": [
    "ans = analyze(models_dict, s_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[\"reweight\"][\"pre\"][\"proxy\"][\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[\"reweight\"][\"pre\"][\"fair\"][\"accuracy\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ced59da7133df83b331c1c27c8f34cf1a7a737ef2fbfbc796b57e0b90e968204"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
